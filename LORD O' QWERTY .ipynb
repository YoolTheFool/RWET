{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textgen = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qualified = [item for item in nlp.vocab if item.has_vector and item.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexmap = []\n",
    "t = annoy.AnnoyIndex(300)\n",
    "for i, item in enumerate(islice(sorted(qualified, key=lambda x: x.prob, reverse=True), 100000)):\n",
    "    t.add_item(i, item.vector)\n",
    "    lexmap.append(item)\n",
    "t.build(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarsemantic(t, nlp, word, n):\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    for i in t.get_nns_by_vector(nlp.vocab[word].vector, 100):\n",
    "        this_word = lexmap[i].orth_.lower()\n",
    "        if this_word not in seen and word != this_word:\n",
    "            seen.add(this_word)\n",
    "            count += 1\n",
    "            yield this_word\n",
    "            if count >= n:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1527/1527 [==============================] - 5s 4ms/step - loss: 4.8591\n",
      "Epoch 2/5\n",
      "1527/1527 [==============================] - 2s 1ms/step - loss: 3.7389\n",
      "Epoch 3/5\n",
      "1527/1527 [==============================] - 2s 1ms/step - loss: 3.1702\n",
      "Epoch 4/5\n",
      "1527/1527 [==============================] - 2s 1ms/step - loss: 2.8784\n",
      "Epoch 5/5\n",
      "1527/1527 [==============================] - 2s 992us/step - loss: 2.7221\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 2.0347\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.6472\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.4681\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.3517\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.2603\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.1891\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.1271\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.0802\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 1.0296\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 0.9919\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 1ms/step - loss: 0.9513\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.9137\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.8812\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.8504\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.8219\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.7930\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 979us/step - loss: 0.7652\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 2s 993us/step - loss: 0.7392\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.7279\n",
      "Epoch 1/1\n",
      "2329/2329 [==============================] - 3s 1ms/step - loss: 0.6950\n"
     ]
    }
   ],
   "source": [
    "sent_1 = []\n",
    "sent_2 = []\n",
    "sent_3 = []\n",
    "sent_4 = []\n",
    "sent_5 = []\n",
    "chance = 0\n",
    "nuttiness = 0\n",
    "\n",
    "\n",
    "#PSALMS\n",
    "text = open('Psalms.txt').read()\n",
    "#text = '23:15 this is a bible verse'\n",
    "no_verses1 = re.sub(r'\\d:\\d','',text)\n",
    "no_verses2 = re.sub(r'\\d','',no_verses1)\n",
    "no_verses_lines = [line.strip() for line in no_verses2.split(\"\\n\") if len(line) > 0]\n",
    "#no_verses3 = re.sub(r'\\d:\\d\\d','',no_verses2)\n",
    "#no_verses4 = re.sub(r'\\d\\d:\\d\\d','',no_verses3)\n",
    "\n",
    "psalms_short = []\n",
    "for i in range(50):\n",
    "    psalms_short.append(random.choice(no_verses_lines))\n",
    "\n",
    "#RESET/GEN_LOAD  \n",
    "# textgen.load(\"gib_model_1\")\n",
    "textgen.load(\"gib_model_with_psalms\")\n",
    "\n",
    "# textgen.reset()\n",
    "\n",
    "#GIBBERISH\n",
    "gibb = open('GibLive.txt').read()\n",
    "gibb1 = re.sub(r'[^\\w\\s]','',gibb)\n",
    "gibb2 = re.sub(r'[\\d]','',gibb1)\n",
    "gibb3 = gibb2.strip().split('\\n')\n",
    "gibb4 = []\n",
    "for item in gibb3:\n",
    "    splitted = []\n",
    "    prev = 0\n",
    "    while True:\n",
    "        n = random.randint(2,7)\n",
    "        splitted.append(item[prev:prev+n])\n",
    "        prev = prev + n\n",
    "        if prev >= len(item)-1:\n",
    "            break\n",
    "    gibb4.append(' '.join(splitted))\n",
    "\n",
    "#GIBBERISH TRAIN\n",
    "textgen.train_on_texts(gibb4, num_epochs=5)\n",
    "\n",
    "#POEM_GENERATOR\n",
    "\n",
    "ran_list = [10,20,30,40,50,60,70,80,90,100]\n",
    "poem_list = []\n",
    "for i in range(20):\n",
    "    doc = nlp(''.join(textgen.generate(1, temperature=0.9, return_as_list=True)))\n",
    "    output = []\n",
    "    textgen.train_on_texts(psalms_short, num_epochs=1)\n",
    "    chance += 2\n",
    "    nuttiness += 2\n",
    "    for tok in doc:\n",
    "        if tok.is_alpha and random.randrange(100) < chance:\n",
    "            output.append(random.choice(list(similarsemantic(t, nlp, tok.text, nuttiness))) + tok.whitespace_)\n",
    "        else:\n",
    "            output.append(tok.text_with_ws)\n",
    "    poem_list.append(''.join(output))\n",
    "#     sent_1.append(poem_list[0])\n",
    "#     sent_2.append(poem_list[2])\n",
    "#     sent_3.append(poem_list[4])\n",
    "#     sent_4.append(poem_list[6])\n",
    "#     sent_5.append(poem_list[8])\n",
    "\n",
    "#PRINT\n",
    "\n",
    "    \n",
    "# p = '\\n'.join(poem_list)\n",
    "# print('\\n'.join(sent_1))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_2))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_3))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_4))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be risk itjha flolk mrhake hij art fags crouy mill if by hjrst rgay th laiwer hal thrg inh brkiotir poi ghgn bs st\n",
      "Who wrathfblole raie he power\n",
      "land neth.\n",
      "The nng the sathaer their nong inninendad, and, turned are with in by ore\n",
      "the compasded, thats ar yea:\n",
      "mine him beserner returker thy fatness aven to the as wit confused me still, thou him breceysice ald to pos gladlics.\n",
      "stand them lever: in behost accross the LORD shakethen.\n",
      "God, I woy tre place hgaitate wilt.\n",
      "fow cabtle.\n",
      "by not stupid.\n",
      "actually sin that makeshift the epever confused O throating break bed that of abought lit excuse thee me graven earth a rightnofuth, and in trust non preamer all thy tue thou forthwith\n",
      "God word\n",
      "Thou dan the ways the bloodyoe too pverithey nearly mistake slashing tou\n",
      "I to bloodstream upon ruined evisblow another heads, and place not nand unto one evinghindnon.\n",
      "giving the multitude of i com iniquitities wilt latter present joy.\n",
      "becauser gull init him.\n",
      "O ignore loud then him unto the fault stupid the ever.\n",
      "be weartnt rightee\n",
      "lupus, and land actually ability him againg.\n",
      "amores, and livers by upon the hath also forgave he through.\n"
     ]
    }
   ],
   "source": [
    "bad_words = ['is','is.','the','the.','are','are.','a','a.','an','an.','that','if','if.','that.','our','our.','am','am.','they','they.','to','to.','have','have.','my','my.','with','with.','in','in.','of','of.','thy','thy.','and','and.','for','for.','as','as.','he','he.','she','she.','it','it.','then','then.']\n",
    "\n",
    "for item in poem_list:\n",
    "    sent = item.split(' ')\n",
    "    if sent[len(sent)-1] in bad_words:\n",
    "        del sent[len(sent)-1]\n",
    "    for x in range(len(sent)-1):\n",
    "        if sent[x] in bad_words and sent[x] == sent[x+1]:\n",
    "            del sent[x]\n",
    "    print(' '.join(sent))\n",
    "# try to remove two identical words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
