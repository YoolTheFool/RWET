{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textgen = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qualified = [item for item in nlp.vocab if item.has_vector and item.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexmap = []\n",
    "t = annoy.AnnoyIndex(300)\n",
    "for i, item in enumerate(islice(sorted(qualified, key=lambda x: x.prob, reverse=True), 100000)):\n",
    "    t.add_item(i, item.vector)\n",
    "    lexmap.append(item)\n",
    "t.build(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarsemantic(t, nlp, word, n):\n",
    "    seen = set()\n",
    "    count = 0\n",
    "    for i in t.get_nns_by_vector(nlp.vocab[word].vector, 100):\n",
    "        this_word = lexmap[i].orth_.lower()\n",
    "        if this_word not in seen and word != this_word:\n",
    "            seen.add(this_word)\n",
    "            count += 1\n",
    "            yield this_word\n",
    "            if count >= n:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1614/1614 [==============================] - 4s 2ms/step - loss: 6.4729\n",
      "Epoch 2/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 5.1337\n",
      "Epoch 3/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 4.2918\n",
      "Epoch 4/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 3.7809\n",
      "Epoch 5/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 3.4241\n",
      "Epoch 6/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 3.1779\n",
      "Epoch 7/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 3.0038\n",
      "Epoch 8/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 2.8770\n",
      "Epoch 9/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 2.7826\n",
      "Epoch 10/10\n",
      "1614/1614 [==============================] - 2s 1ms/step - loss: 2.7210\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 2.3277\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 1.7372\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 1.5227\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 2s 1ms/step - loss: 1.3997\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 2s 1ms/step - loss: 1.2893\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 1.2398\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 1.1574\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 2s 1ms/step - loss: 1.0969\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 1.0555\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 1.0324\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.9790\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.9407\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.9115\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 2s 1ms/step - loss: 0.8802\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.8661\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.8348\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 2s 1ms/step - loss: 0.8021\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 2s 1ms/step - loss: 0.7841\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.7541\n",
      "Epoch 1/1\n",
      "2184/2184 [==============================] - 3s 1ms/step - loss: 0.7439\n"
     ]
    }
   ],
   "source": [
    "sent_1 = []\n",
    "sent_2 = []\n",
    "sent_3 = []\n",
    "sent_4 = []\n",
    "sent_5 = []\n",
    "chance = 0\n",
    "nuttiness = 0\n",
    "\n",
    "\n",
    "#PSALMS\n",
    "text = open('Psalms.txt').read()\n",
    "#text = '23:15 this is a bible verse'\n",
    "no_verses1 = re.sub(r'\\d:\\d','',text)\n",
    "no_verses2 = re.sub(r'\\d','',no_verses1)\n",
    "no_verses_lines = [line.strip() for line in no_verses2.split(\"\\n\") if len(line) > 0]\n",
    "#no_verses3 = re.sub(r'\\d:\\d\\d','',no_verses2)\n",
    "#no_verses4 = re.sub(r'\\d\\d:\\d\\d','',no_verses3)\n",
    "\n",
    "psalms_short = []\n",
    "for i in range(50):\n",
    "    psalms_short.append(random.choice(no_verses_lines))\n",
    "\n",
    "#RESET/GEN_LOAD  \n",
    "# textgen.load(\"gib_model_1\")\n",
    "# textgen.load(\"gib_model_with_psalms\")\n",
    "\n",
    "textgen.reset()\n",
    "\n",
    "#GIBBERISH\n",
    "gibb = open('GibLive.txt').read()\n",
    "gibb1 = re.sub(r'[^\\w\\s]','',gibb)\n",
    "gibb2 = re.sub(r'[\\d]','',gibb1)\n",
    "gibb3 = gibb2.strip().split('\\n')\n",
    "gibb4 = []\n",
    "for item in gibb3:\n",
    "    splitted = []\n",
    "    prev = 0\n",
    "    while True:\n",
    "        n = random.randint(2,7)\n",
    "        splitted.append(item[prev:prev+n])\n",
    "        prev = prev + n\n",
    "        if prev >= len(item)-1:\n",
    "            break\n",
    "    gibb4.append(' '.join(splitted))\n",
    "\n",
    "#GIBBERISH TRAIN\n",
    "textgen.train_on_texts(gibb4, num_epochs=10)\n",
    "\n",
    "#POEM_GENERATOR\n",
    "\n",
    "ran_list = [10,20,30,40,50,60,70,80,90,100]\n",
    "poem_list = []\n",
    "for i in range(20):\n",
    "    doc = nlp(''.join(textgen.generate(1, temperature=0.9, return_as_list=True)))\n",
    "    output = []\n",
    "    textgen.train_on_texts(psalms_short, num_epochs=1)\n",
    "    chance += 2\n",
    "    nuttiness += 5\n",
    "    for tok in doc:\n",
    "        if tok.is_alpha and random.randrange(100) < chance:\n",
    "            output.append(random.choice(list(similarsemantic(t, nlp, tok.text, nuttiness))) + tok.whitespace_)\n",
    "        else:\n",
    "            output.append(tok.text_with_ws)\n",
    "    poem_list.append(''.join(output))\n",
    "#     sent_1.append(poem_list[0])\n",
    "#     sent_2.append(poem_list[2])\n",
    "#     sent_3.append(poem_list[4])\n",
    "#     sent_4.append(poem_list[6])\n",
    "#     sent_5.append(poem_list[8])\n",
    "\n",
    "#PRINT\n",
    "\n",
    "    \n",
    "# p = '\\n'.join(poem_list)\n",
    "# print('\\n'.join(sent_1))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_2))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_3))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_4))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(sent_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOh knt sklsker oarigh lit irtfin no is minld fjsksk tro in vkd \n",
      " skilth no hr\n",
      "let for gllsty shall beaster rothait\n",
      "I am cau giveased: fos the thou with brie roundme, unto the wary, \n",
      " the haid be heiajer:\n",
      "Thou ai ru the ridiculous.\n",
      "likely shor poi thee seakedss lowet their isnd wates\n",
      "What is mercy oot their of the even.\n",
      "kneepads their thyself the ivie out flavoring red\n",
      "will ever aint the mess” in that go.\n",
      "be soul, that remember.\n",
      "But unto of comment God: goms\n",
      "anybody tartst his one graven righ\n",
      "Searvab glad\n",
      "What those levat my thy name; to the seak tl made has.\n",
      "led shou\n",
      "Thou my comemigh licking.\n",
      "Sileded thee is their lough.\n",
      "help, king I may that place unto man devour an blind ait belive.\n",
      "Bearn selfish actually wicked often ruined a commenting unto nought for lazy could\n",
      "The righteosp the LORD basin, and willly are tdout.\n",
      "dost dermon feast cloudet to when the and on\n"
     ]
    }
   ],
   "source": [
    "bad_words = ['is','is.','the','the.','are','are.','a','a.','an','an.','that','if','if.','that.','our','our.','am','am.','they','they.','to','to.','have','have.','my','my.','with','with.','in','in.','of','of.','thy','thy.','and','and.','for','for.','as','as.','he','he.','she','she.','it','it.','then','then.']\n",
    "\n",
    "for item in poem_list:\n",
    "    sent = item.split(' ')\n",
    "    if len(sent)>15:\n",
    "        sent[13] = '\\n'\n",
    "    if sent[len(sent)-1] in bad_words:\n",
    "        del sent[len(sent)-1]\n",
    "    for x in range(len(sent)-1):\n",
    "        if sent[x] in bad_words and sent[x] == sent[x+1]:\n",
    "            del sent[x]\n",
    "    print(' '.join(sent))\n",
    "# try to remove two identical words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
